{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do Cuda: 2.9.0+cu130\n",
      "Disponibilidade da GPU: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "print(f'Versão do Cuda: {torch.__version__}')\n",
    "print(f'Disponibilidade da GPU: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bd7f9",
   "metadata": {},
   "source": [
    "# V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20911e60",
   "metadata": {},
   "source": [
    "##### Ajuste dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a70a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceFeatureDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=8, latent_dim=2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "CSV_PATH = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\joao\\data\\features_all.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "X = df.drop(columns=['ID', 'Age', 'Sex', 'Class']).values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "dataset = VoiceFeatureDataset(X_scaled)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a1b2f",
   "metadata": {},
   "source": [
    "##### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8de4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Autoencoder(input_dim=8, latent_dim=3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 100\n",
    "losses = []\n",
    "pseudo_accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for features in dataloader:\n",
    "        features = features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon, z = model(features)\n",
    "        loss = criterion(recon, features)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Pseudo-accuracy baseado no MSE\n",
    "        batch_acc = 1 - torch.mean((recon - features)**2).item()\n",
    "        total_acc += batch_acc\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_acc = total_acc / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    pseudo_accs.append(avg_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{n_epochs} | Loss: {avg_loss:.6f}  | Accuracy: {avg_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4508c",
   "metadata": {},
   "source": [
    "##### Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e882719",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(losses, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Loss\")\n",
    "plt.title(\"Loss por Época\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(pseudo_accs, marker='o', color='green')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Pseudo-Accuracy\")\n",
    "plt.title(\"Acurácia Relativa por Época\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        x = dataset[i].unsqueeze(0).to(device)\n",
    "        _, z = model(x)\n",
    "        embeddings.append(z.cpu().numpy().flatten())\n",
    "\n",
    "embeddings_df = pd.DataFrame(embeddings, columns=['z1', 'z2', 'z3'])\n",
    "result = pd.concat([df[['ID', 'Age', 'Sex', 'Class']], embeddings_df], axis=1)\n",
    "result.to_csv(r\"C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\joao\\data\\voice_embeddings.csv\", index=False)\n",
    "print(result.head())\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "classes = sorted(result['Class'].unique())\n",
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    subset = result[result['Class'] == cls]\n",
    "    ax.scatter(subset['z1'], subset['z2'], subset['z3'], \n",
    "    label=f\"Class {cls}\", alpha=0.7, color=colors[i % len(colors)])\n",
    "\n",
    "ax.set_xlabel(\"z1\")\n",
    "ax.set_ylabel(\"z2\")\n",
    "ax.set_zlabel(\"z3\")\n",
    "ax.set_title(\"Latent Space\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06d4d6",
   "metadata": {},
   "source": [
    "O objetivo principal de analisar o espaço latente é ver se o modelo aprendeu a representar classes diferentes em regiões distintas. \n",
    "\n",
    "<b>Separação entre classes</b>\n",
    "\n",
    "Como podem ver, as cores estão misturadas, o que indica que o espaço latente não está separando bem as classes. Isso sugere que o modelo ainda não capturou bem as características discriminativas entre as classes.\n",
    "\n",
    "<b>Distribuição e estrutura dos dados</b>\n",
    "\n",
    "A maior concentração dos pontos está perto da origem (em torno de (0,0) até (-2, 2)), o que é comum em autoencoders, pois o modelo tende a comprimir as representações para regiões centrais.\n",
    "Há alguns pontos fora desse núcleo, que provavelmente são os outliers ou amostras com características incomuns no conjunto. Esses pontos isolados às vezes revelam anomalias ou exemplos mal reconstruídos.\n",
    "\n",
    "<b>Densidade e sobreposição</b>\n",
    "\n",
    "A sobreposição de classes mostra que o modelo pode estar aprendendo características gerais, mas não específicas por classe ou que as classes não são facilmente separáveis nos dados originais.\n",
    "\n",
    "Temos duas hipóteses para isso:\n",
    "\n",
    "1. As classes realmente compartilham características semelhantes (pouca separabilidade nos dados brutos);\n",
    "\n",
    "2. O modelo tem capacidade insuficiente ou regularização excessiva, impedindo-o de criar fronteiras latentes mais distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425d16b",
   "metadata": {},
   "source": [
    "# V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e34d0",
   "metadata": {},
   "source": [
    "### Curiosidades\n",
    "\n",
    "<b>MFCC</b>\n",
    "\n",
    "No processamento de áudio, o cepstro de frequência Mel (MFC) é uma representação do espectro de potência de curto prazo de um som, obtida a partir de uma transformação discreta do espectro de potência logarítmico, mapeado em uma escala Mel, que é uma escala de frequência não linear inspirada na percepção auditiva humana.\n",
    "\n",
    "Os coeficientes cepstrais de frequência Mel (MFCCs) são os valores que, em conjunto, formam um MFC. Eles são derivados de uma representação cepstral do áudio — ou seja, do “espectro do espectro” de forma não linear. A principal diferença entre o cepstro tradicional e o cepstro de frequência Mel é que, no MFC, as bandas de frequência são distribuídas de forma uniforme na escala Mel, aproximando-se mais da forma como o ouvido humano percebe o som, ao invés do espaçamento linear das frequências no espectro convencional. Essa adaptação melhora a representação do som, sendo especialmente útil em aplicações como compressão de áudio, reconhecimento de fala e análise acústica, reduzindo a largura de banda necessária e melhorando a eficiência do processamento.\n",
    "\n",
    "O cálculo dos MFCCs geralmente segue os seguintes passos:\n",
    "\n",
    "- Janela temporal e Transformada de Fourier (FFT): O sinal de áudio é dividido em trechos curtos (janelas), e para cada trecho é calculada a transformada de Fourier, obtendo-se o espectro de potência.\n",
    "\n",
    "- Mapeamento na escala Mel: As potências do espectro são projetadas na escala Mel, utilizando bancos de filtros triangulares sobrepostos ou, alternativamente, janelas cosseno sobrepostas.\n",
    "\n",
    "- Logaritmo das potências: Calcula-se o logaritmo das potências em cada faixa de frequência Mel, aproximando a percepção humana de intensidade sonora.\n",
    "\n",
    "- Transformada Discreta de Cosseno (DCT): Aplica-se a DCT sobre a lista de potências logaritmizadas, obtendo um espectro que representa a envoltória do espectro original.\n",
    "\n",
    "- Os MFCCs correspondem às amplitudes do espectro resultante e formam a base de muitas técnicas de análise de áudio e reconhecimento de padrões acústicos.\n",
    "\n",
    "<b>BATCH SIZE</b>\n",
    "\n",
    "O `batch size` é um dos hiperparâmetros mais importantes no treinamento de redes neurais.\n",
    "\n",
    "Durante o treinamento, sua rede não atualiza os pesos após ver cada exemplo individualmente, nem espera ver todos os exemplos do dataset de uma vez só, pois isso seria muito pesado para processar. Em vez disso, o conjunto de dados é dividido em pequenos lotes, e a rede:\n",
    "\n",
    "- Faz uma passagem direta (forward) com todos os exemplos do lote.\n",
    "- Calcula a média do erro (loss) do lote.\n",
    "- Faz uma passagem reversa (backpropagation) para atualizar os pesos com base nesse erro médio.\n",
    "\n",
    "Suponha que você tenha 1.600 áudios e define: BATCH_SIZE = 16\n",
    "\n",
    "O modelo vai dividir o dataset em:\n",
    "\n",
    "1600/16 = 100 batches por época\n",
    "\n",
    "Cada batch contém 16 exemplos, e a cada lote processado o modelo ajusta os pesos uma vez.\n",
    "\n",
    "| Tamanho do Batch\t        | Características |\n",
    "| ------------------------- | --------------- |\n",
    "| Pequeno (ex: 8, 16)\t    | Atualizações mais frequentes → aprendizado mais “instável” mas às vezes melhor generalização        |\n",
    "| Grande (ex: 64, 128, 256) | Atualizações mais suaves e estáveis, mas requer mais memória e pode “ficar preso” em mínimos locais |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9540b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\joao\\data\\task1\\training\\phonationA'\n",
    "LABELS_PATH = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\joao\\data\\task1\\labels.csv'\n",
    "SAMPLE_RATE = 8000   # Taxa de amostragem dos áudios\n",
    "N_MFCC = 13          # Número de coeficientes MFCCs\n",
    "LATENT_DIM = 20      # Dimensões do espaço latente\n",
    "BATCH_SIZE = 64      # Tamanho do lote\n",
    "N_EPOCHS = 2000      # Número de épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureDataset(Dataset):\n",
    "    def __init__(self, audio_files, csv_path, sample_rate=8000, n_mfcc=13, scaler=None):\n",
    "        self.audio_files = audio_files\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.scaler = scaler\n",
    "\n",
    "        self.df_labels = pd.read_csv(csv_path)\n",
    "        self.id_to_class = {row['ID']: int(row['Class']) for _, row in self.df_labels.iterrows()}\n",
    "\n",
    "        self.features = []\n",
    "        self.classes = []\n",
    "\n",
    "        all_features = []\n",
    "\n",
    "        for file in self.audio_files:\n",
    "            mfccs = self.extract_mfcc(file)\n",
    "            if mfccs is not None:\n",
    "                self.features.append(mfccs)\n",
    "                all_features.append(mfccs)\n",
    "\n",
    "                file_id = os.path.basename(file).split('_')[0]\n",
    "                if file_id not in self.id_to_class:\n",
    "                    raise ValueError(f\"ID {file_id} não encontrado no CSV\")\n",
    "                self.classes.append(self.id_to_class[file_id])\n",
    "\n",
    "        if self.scaler is None:\n",
    "            all_frames = np.concatenate(all_features, axis=0)\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(all_frames)\n",
    "            print(\"Scaler ajustado com sucesso.\")\n",
    "\n",
    "        self.normalized_features = [\n",
    "            torch.tensor(self.scaler.transform(f), dtype=torch.float32)\n",
    "            for f in self.features\n",
    "        ]\n",
    "\n",
    "        print(f\"Total de {len(self.normalized_features)} áudios carregados e normalizados.\")\n",
    "\n",
    "    def extract_mfcc(self, file_path):\n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=self.n_mfcc)\n",
    "            return mfccs.T\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.normalized_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.normalized_features[idx]\n",
    "        length = features.shape[0]\n",
    "        classe = self.classes[idx]\n",
    "        return features, length, classe\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, lengths, classes = zip(*batch)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    padded_batch = pad_sequence(sequences, batch_first=True)\n",
    "    \n",
    "    lengths, sorted_idx = lengths.sort(descending=True)\n",
    "    padded_batch = padded_batch[sorted_idx]\n",
    "    classes = torch.tensor([classes[i] for i in sorted_idx], dtype=torch.long)\n",
    "\n",
    "    return padded_batch, lengths, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9860f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder_linear = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.decoder_linear = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_linear = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, max_len, _ = x.size()\n",
    "        \n",
    "        packed_input = pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
    "        \n",
    "        _, (h_n, c_n) = self.encoder_lstm(packed_input)\n",
    "        z = self.encoder_linear(h_n[-1])\n",
    "        \n",
    "        h_0_decoder = self.decoder_linear(z).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c_0_decoder = torch.zeros_like(h_0_decoder)\n",
    "        \n",
    "        packed_input_decoder = pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
    "        packed_output, _ = self.decoder_lstm(packed_input_decoder, (h_0_decoder, c_0_decoder))\n",
    "        \n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=max_len)\n",
    "        x_recon = self.output_linear(output)\n",
    "        \n",
    "        return x_recon, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea10148",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio_files = glob.glob(os.path.join(AUDIO_DIR, \"*.wav\"))\n",
    "\n",
    "if not all_audio_files:\n",
    "    print(f\"Nenhum arquivo de áudio encontrado em {AUDIO_DIR}. Verifique o caminho e a extensão.\")\n",
    "else:\n",
    "    print(f\"Encontrados {len(all_audio_files)} arquivos de áudio.\")\n",
    "    \n",
    "    dataset = AudioFeatureDataset(all_audio_files, csv_path=LABELS_PATH)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTMAutoencoder(input_dim=N_MFCC, hidden_dim=64, latent_dim=LATENT_DIM).to(device)\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_history = []\n",
    "    print(\"Iniciando treinamento...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        total_loss = 0\n",
    "        for batch, lengths, classes in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, z = model(batch, lengths)\n",
    "            \n",
    "            mask = torch.zeros_like(batch, dtype=torch.bool).to(device)\n",
    "            for i, l in enumerate(lengths):\n",
    "                mask[i, :l, :] = True\n",
    "            \n",
    "            loss_all = criterion(recon, batch)\n",
    "            loss_masked = loss_all * mask.float()\n",
    "            loss = loss_masked.sum() / mask.sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * lengths.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS} | Loss: {avg_loss:.6f}\")\n",
    "                \n",
    "    print(\"Treinamento concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, N_EPOCHS+1), loss_history, marker=',')\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals() and LATENT_DIM in [2, 3]:\n",
    "    print(\"Extraindo vetores latentes...\")\n",
    "    \n",
    "    model.eval()\n",
    "    latent_vectors = []\n",
    "    all_classes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print(\"latent_vectors shape (antes de concatenar):\", [v.shape for v in latent_vectors])\n",
    "        print(\"classes shape (antes de concatenar):\", [c.shape for c in all_classes])\n",
    "        print(\"Exemplo de classes:\", all_classes[0] if all_classes else \"vazio\")\n",
    "        \n",
    "        for batch, lengths, classes in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            _, z = model(batch, lengths)\n",
    "            latent_vectors.append(z.cpu().numpy())\n",
    "            all_classes.append(classes.cpu().numpy())\n",
    "            \n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "    all_classes = np.concatenate(all_classes, axis=0)\n",
    "    \n",
    "    \n",
    "    unique_classes = np.unique(all_classes)\n",
    "    cmap = plt.cm.get_cmap('tab10', len(unique_classes))\n",
    "    class_to_idx = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "    colors = [cmap(class_to_idx[cls]) for cls in all_classes]\n",
    "        \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(latent_vectors[:, 0], latent_vectors[:, 1], latent_vectors[:, 2], c=colors, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlim([-0.7, -0.55])\n",
    "    ax.set_ylim([2.05, 2.35])\n",
    "    ax.set_zlim([1.9, 2.6])\n",
    "\n",
    "    ax.set_title('Latent Space 3d')\n",
    "    ax.set_xlabel('z1')\n",
    "    ax.set_ylabel('z2')\n",
    "    ax.set_zlabel('z3')\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(class_to_idx[cls]), markersize=10, label=f'Classe {cls}') for cls in unique_classes]\n",
    "    ax.legend(handles=handles, title=\"Classes\")\n",
    "    plt.show()\n",
    "        \n",
    "    print(f'Plotagem concluída. Total de {len(latent_vectors)} vetores latentes extraídos.')\n",
    "else:\n",
    "    print('Plotagem do espaço latente não realizada.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
