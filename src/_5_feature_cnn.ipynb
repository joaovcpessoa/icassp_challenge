{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do Cuda: 2.9.0+cu130\n",
      "Disponibilidade da GPU: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import zipfile\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "import librosa\n",
    "import parselmouth\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "from typing import List, Dict, Tuple, Callable, Any\n",
    "\n",
    "print(f'Versão do Cuda: {torch.__version__}')\n",
    "print(f'Disponibilidade da GPU: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd551af",
   "metadata": {},
   "source": [
    "##### CNN multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data'\n",
    "INPUT_FILES = [\n",
    "    'features_z_score.csv',\n",
    "    'features_min_max_0_1.csv',\n",
    "    'features_signal_norm_-1_1_z_score.csv',\n",
    "    'features_signal_norm_-1_1_min_max_0_1.csv'\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "KFOLD_SPLITS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].unsqueeze(0), self.y[idx]\n",
    "\n",
    "class MulticlassCNN(nn.Module):\n",
    "    # Pooling assimétrico\n",
    "    # dict = {'ID001': [AGE, SEX,...], 'ID002': []}\n",
    "    def __init__(self, input_dim: int, num_classes: int):\n",
    "        super(MulticlassCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layer = nn.Sequential(\n",
    "            # Camada 1: 1 -> 32 canais. Dimensão / 2\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), \n",
    "            \n",
    "            # Camada 2: 32 -> 64 canais. Dimensão / 4\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Camada 3: 64 -> 128 canais. Dimensão / 8\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Camada 4: 128 -> 256 canais. Dimensão / 16\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Camada 5: 256 -> 512 canais. Dimensão / 32\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        flatten_size = 512 * (input_dim // 32) \n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(flatten_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), \n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "def train_and_evaluate(df: pd.DataFrame, filename: str) -> Dict[str, float]:\n",
    "    print(f\"\\n{'='*50}\\nIniciando treinamento para: {filename}\\n{'='*50}\")\n",
    "\n",
    "    sex_mapping = {'M': 0, 'F': 1}\n",
    "    df['Sex'] = df['Sex'].map(sex_mapping)\n",
    "    \n",
    "    X_raw = df.drop(columns=['ID', 'Class']).values\n",
    "    y_raw = df['Class'].values\n",
    "    \n",
    "    X = np.nan_to_num(X_raw, nan=0.0) \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_raw)\n",
    "    \n",
    "    INPUT_DIM = X.shape[1]\n",
    "    NUM_CLASSES = len(np.unique(y))\n",
    "    \n",
    "    print(f\"Dimensão da entrada (Input_dim): {INPUT_DIM}\")\n",
    "    print(f\"Número de classes: {NUM_CLASSES}\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=42)\n",
    "    fold_results = {'accuracy': [], 'f1_weighted': []}\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        \n",
    "        print(f\"\\n--- Fold {fold+1}/{KFOLD_SPLITS} ---\")\n",
    "\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        train_dataset = FeatureDataset(X_train, y_train)\n",
    "        val_dataset = FeatureDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = MulticlassCNN(INPUT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for features, labels in train_loader:\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Fold {fold+1}, Epoch {epoch+1}/{N_EPOCHS}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1_w = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        fold_results['accuracy'].append(acc)\n",
    "        fold_results['f1_weighted'].append(f1_w)\n",
    "        \n",
    "        print(f\"Resultado do Fold {fold+1}: Accuracy = {acc:.4f}, F1-Weighted = {f1_w:.4f}\")\n",
    "\n",
    "    final_results = {\n",
    "        'Filename': filename,\n",
    "        'Mean_Accuracy': np.mean(fold_results['accuracy']),\n",
    "        'Std_Accuracy': np.std(fold_results['accuracy']),\n",
    "        'Mean_F1_Weighted': np.mean(fold_results['f1_weighted']),\n",
    "        'Std_F1_Weighted': np.std(fold_results['f1_weighted'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResumo Final ({filename}):\")\n",
    "    print(f\"  Média Accuracy: {final_results['Mean_Accuracy']:.4f} (+/- {final_results['Std_Accuracy']:.4f})\")\n",
    "    print(f\"  Média F1-W: {final_results['Mean_F1_Weighted']:.4f} (+/- {final_results['Std_F1_Weighted']:.4f})\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "missing_files = [f for f in INPUT_FILES if not os.path.exists(os.path.join(OUTPUT_DIR, f))]\n",
    "if missing_files:\n",
    "    print(f\"ERRO: Os seguintes arquivos de entrada não foram encontrados no diretório {OUTPUT_DIR}:\")\n",
    "    for f in missing_files:\n",
    "        print(f\"- {f}\")\n",
    "    print(\"\\nCertifique-se de executar o script de extração e normalização primeiro.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "all_experiment_results = []\n",
    "\n",
    "for filename in INPUT_FILES:\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        results = train_and_evaluate(df, filename)\n",
    "        all_experiment_results.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o arquivo {filename}: {e}\", file=sys.stderr)\n",
    "\n",
    "results_df = pd.DataFrame(all_experiment_results)\n",
    "output_results_path = os.path.join(OUTPUT_DIR, 'cnn_multiclass_results.csv')\n",
    "results_df.to_csv(output_results_path, index=False)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"RESUMO FINAL DOS EXPERIMENTOS\")\n",
    "print(\"=\"*70)\n",
    "print(results_df)\n",
    "print(f\"\\nResultados consolidados salvos em: {output_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05468d77",
   "metadata": {},
   "source": [
    "##### One-vs-Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454aa811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Iniciando Pipeline One-vs-Rest (OvR) para: features_z_score.csv\n",
      "======================================================================\n",
      "  Dimensão de Features (X): (272, 186)\n",
      "\n",
      "--- OvR_Class_1 (vs Restante) ---\n",
      "  Accuracy: 0.9780, F1-W: 0.9671\n",
      "\n",
      "--- OvR_Class_2 (vs Restante) ---\n",
      "  Accuracy: 0.9046, F1-W: 0.8805\n",
      "\n",
      "--- OvR_Class_3 (vs Restante) ---\n",
      "  Accuracy: 0.6910, F1-W: 0.6759\n",
      "\n",
      "--- OvR_Class_4 (vs Restante) ---\n",
      "  Accuracy: 0.6247, F1-W: 0.6076\n",
      "\n",
      "--- OvR_Class_5 (vs Restante) ---\n",
      "  Accuracy: 0.6323, F1-W: 0.6251\n",
      "\n",
      "######################################################################\n",
      "MATRIZ DE CONFUSÃO (5x5) OVR - Agregação de K-Fold para: features_z_score.csv\n",
      "Rótulos: [1, 2, 3, 4, 5]\n",
      "######################################################################\n",
      "                 Pred 1   Pred 2   Pred 3   Pred 4   Pred 5\n",
      "-------------------------------------------------------\n",
      "Classe 1 |            0       1       1       3       1\n",
      "Classe 2 |            1       5      11       3       6\n",
      "Classe 3 |            2       3      14      18      20\n",
      "Classe 4 |            0       3      12      26      35\n",
      "Classe 5 |            0       2      16      28      61\n",
      "######################################################################\n",
      "Matriz de confusão salva em: C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data\\confusion_matrix_features_z_score.png\n",
      "\n",
      "======================================================================\n",
      "Iniciando Pipeline One-vs-Rest (OvR) para: features_min_max_0_1.csv\n",
      "======================================================================\n",
      "  Dimensão de Features (X): (272, 186)\n",
      "\n",
      "--- OvR_Class_1 (vs Restante) ---\n",
      "  Accuracy: 0.9780, F1-W: 0.9671\n",
      "\n",
      "--- OvR_Class_2 (vs Restante) ---\n",
      "  Accuracy: 0.8677, F1-W: 0.8520\n",
      "\n",
      "--- OvR_Class_3 (vs Restante) ---\n",
      "  Accuracy: 0.7648, F1-W: 0.7502\n",
      "\n",
      "--- OvR_Class_4 (vs Restante) ---\n",
      "  Accuracy: 0.6652, F1-W: 0.6408\n",
      "\n",
      "--- OvR_Class_5 (vs Restante) ---\n",
      "  Accuracy: 0.6142, F1-W: 0.5966\n",
      "\n",
      "######################################################################\n",
      "MATRIZ DE CONFUSÃO (5x5) OVR - Agregação de K-Fold para: features_min_max_0_1.csv\n",
      "Rótulos: [1, 2, 3, 4, 5]\n",
      "######################################################################\n",
      "                 Pred 1   Pred 2   Pred 3   Pred 4   Pred 5\n",
      "-------------------------------------------------------\n",
      "Classe 1 |            0       2       2       1       1\n",
      "Classe 2 |            2       4      11       5       4\n",
      "Classe 3 |            2       6      20      13      16\n",
      "Classe 4 |            0       5      10      27      34\n",
      "Classe 5 |            0       1      14      32      60\n",
      "######################################################################\n",
      "Matriz de confusão salva em: C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data\\confusion_matrix_features_min_max_0_1.png\n",
      "\n",
      "======================================================================\n",
      "Iniciando Pipeline One-vs-Rest (OvR) para: features_signal_norm_-1_1_z_score.csv\n",
      "======================================================================\n",
      "  Dimensão de Features (X): (272, 186)\n",
      "\n",
      "--- OvR_Class_1 (vs Restante) ---\n",
      "  Accuracy: 0.9706, F1-W: 0.9633\n",
      "\n",
      "--- OvR_Class_2 (vs Restante) ---\n",
      "  Accuracy: 0.8675, F1-W: 0.8589\n",
      "\n",
      "--- OvR_Class_3 (vs Restante) ---\n",
      "  Accuracy: 0.7204, F1-W: 0.7114\n",
      "\n",
      "--- OvR_Class_4 (vs Restante) ---\n",
      "  Accuracy: 0.6028, F1-W: 0.5893\n",
      "\n",
      "--- OvR_Class_5 (vs Restante) ---\n",
      "  Accuracy: 0.6211, F1-W: 0.6203\n",
      "\n",
      "######################################################################\n",
      "MATRIZ DE CONFUSÃO (5x5) OVR - Agregação de K-Fold para: features_signal_norm_-1_1_z_score.csv\n",
      "Rótulos: [1, 2, 3, 4, 5]\n",
      "######################################################################\n",
      "                 Pred 1   Pred 2   Pred 3   Pred 4   Pred 5\n",
      "-------------------------------------------------------\n",
      "Classe 1 |            1       1       2       1       1\n",
      "Classe 2 |            0       7       7       6       6\n",
      "Classe 3 |            1       4      15      19      18\n",
      "Classe 4 |            1       5      17      19      34\n",
      "Classe 5 |            0       0      16      34      57\n",
      "######################################################################\n",
      "Matriz de confusão salva em: C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data\\confusion_matrix_features_signal_norm_-1_1_z_score.png\n",
      "\n",
      "======================================================================\n",
      "Iniciando Pipeline One-vs-Rest (OvR) para: features_signal_norm_-1_1_min_max_0_1.csv\n",
      "======================================================================\n",
      "  Dimensão de Features (X): (272, 186)\n",
      "\n",
      "--- OvR_Class_1 (vs Restante) ---\n",
      "  Accuracy: 0.9780, F1-W: 0.9671\n",
      "\n",
      "--- OvR_Class_2 (vs Restante) ---\n",
      "  Accuracy: 0.8898, F1-W: 0.8731\n",
      "\n",
      "--- OvR_Class_3 (vs Restante) ---\n",
      "  Accuracy: 0.6733, F1-W: 0.6743\n",
      "\n",
      "--- OvR_Class_4 (vs Restante) ---\n",
      "  Accuracy: 0.6288, F1-W: 0.6192\n",
      "\n",
      "--- OvR_Class_5 (vs Restante) ---\n",
      "  Accuracy: 0.5327, F1-W: 0.5328\n",
      "\n",
      "######################################################################\n",
      "MATRIZ DE CONFUSÃO (5x5) OVR - Agregação de K-Fold para: features_signal_norm_-1_1_min_max_0_1.csv\n",
      "Rótulos: [1, 2, 3, 4, 5]\n",
      "######################################################################\n",
      "                 Pred 1   Pred 2   Pred 3   Pred 4   Pred 5\n",
      "-------------------------------------------------------\n",
      "Classe 1 |            0       2       1       2       1\n",
      "Classe 2 |            0       8       8       6       4\n",
      "Classe 3 |            0       8      18      13      18\n",
      "Classe 4 |            0       5      12      21      38\n",
      "Classe 5 |            0       6      21      33      47\n",
      "######################################################################\n",
      "Matriz de confusão salva em: C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data\\confusion_matrix_features_signal_norm_-1_1_min_max_0_1.png\n",
      "\n",
      "######################################################################\n",
      "MATRIZ DE CONFUSÃO (5x5) OVR - Agregação de K-Fold para: features_z_score.csv\n",
      "Rótulos: [1, 2, 3, 4, 5]\n",
      "######################################################################\n",
      "                 Pred 1   Pred 2   Pred 3   Pred 4   Pred 5\n",
      "-------------------------------------------------------\n",
      "Classe 1 |            0       1       1       3       1\n",
      "Classe 2 |            1       5      11       3       6\n",
      "Classe 3 |            2       3      14      18      20\n",
      "Classe 4 |            0       3      12      26      35\n",
      "Classe 5 |            0       2      16      28      61\n",
      "######################################################################\n",
      "Matriz de confusão salva em: C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data\\confusion_matrix_features_z_score.png\n",
      "\n",
      "\n",
      "======================================================================\n",
      "RESUMO FINAL DOS EXPERIMENTOS ONE-VS-REST (OvR)\n",
      "======================================================================\n",
      "    Target_Class  Mean_Accuracy  Std_Accuracy  Mean_F1_Weighted  \\\n",
      "5              1       0.977980      0.007173          0.967106   \n",
      "6              2       0.867677      0.035618          0.852007   \n",
      "7              3       0.764781      0.037191          0.750155   \n",
      "8              4       0.665185      0.050230          0.640842   \n",
      "9              5       0.614209      0.045794          0.596639   \n",
      "15             1       0.977980      0.007173          0.967106   \n",
      "16             2       0.889764      0.030562          0.873091   \n",
      "17             3       0.673333      0.080985          0.674256   \n",
      "18             4       0.628754      0.082468          0.619193   \n",
      "19             5       0.532727      0.050468          0.532793   \n",
      "10             1       0.970572      0.014800          0.963332   \n",
      "11             2       0.867475      0.022434          0.858915   \n",
      "12             3       0.720404      0.038869          0.711368   \n",
      "13             4       0.602828      0.035139          0.589306   \n",
      "14             5       0.621077      0.034627          0.620328   \n",
      "0              1       0.977980      0.007173          0.967106   \n",
      "1              2       0.904646      0.028573          0.880533   \n",
      "2              3       0.691044      0.019877          0.675917   \n",
      "3              4       0.624714      0.063307          0.607584   \n",
      "4              5       0.632256      0.031906          0.625112   \n",
      "\n",
      "    Std_F1_Weighted                                   Filename  \\\n",
      "5          0.010659                   features_min_max_0_1.csv   \n",
      "6          0.025474                   features_min_max_0_1.csv   \n",
      "7          0.037206                   features_min_max_0_1.csv   \n",
      "8          0.046501                   features_min_max_0_1.csv   \n",
      "9          0.050164                   features_min_max_0_1.csv   \n",
      "15         0.010659  features_signal_norm_-1_1_min_max_0_1.csv   \n",
      "16         0.027277  features_signal_norm_-1_1_min_max_0_1.csv   \n",
      "17         0.065900  features_signal_norm_-1_1_min_max_0_1.csv   \n",
      "18         0.075069  features_signal_norm_-1_1_min_max_0_1.csv   \n",
      "19         0.047207  features_signal_norm_-1_1_min_max_0_1.csv   \n",
      "10         0.011459      features_signal_norm_-1_1_z_score.csv   \n",
      "11         0.018107      features_signal_norm_-1_1_z_score.csv   \n",
      "12         0.029420      features_signal_norm_-1_1_z_score.csv   \n",
      "13         0.035659      features_signal_norm_-1_1_z_score.csv   \n",
      "14         0.036035      features_signal_norm_-1_1_z_score.csv   \n",
      "0          0.010659                       features_z_score.csv   \n",
      "1          0.037346                       features_z_score.csv   \n",
      "2          0.023667                       features_z_score.csv   \n",
      "3          0.066180                       features_z_score.csv   \n",
      "4          0.029257                       features_z_score.csv   \n",
      "\n",
      "                   Classifier  \n",
      "5   OvR_Class_1 (vs Restante)  \n",
      "6   OvR_Class_2 (vs Restante)  \n",
      "7   OvR_Class_3 (vs Restante)  \n",
      "8   OvR_Class_4 (vs Restante)  \n",
      "9   OvR_Class_5 (vs Restante)  \n",
      "15  OvR_Class_1 (vs Restante)  \n",
      "16  OvR_Class_2 (vs Restante)  \n",
      "17  OvR_Class_3 (vs Restante)  \n",
      "18  OvR_Class_4 (vs Restante)  \n",
      "19  OvR_Class_5 (vs Restante)  \n",
      "10  OvR_Class_1 (vs Restante)  \n",
      "11  OvR_Class_2 (vs Restante)  \n",
      "12  OvR_Class_3 (vs Restante)  \n",
      "13  OvR_Class_4 (vs Restante)  \n",
      "14  OvR_Class_5 (vs Restante)  \n",
      "0   OvR_Class_1 (vs Restante)  \n",
      "1   OvR_Class_2 (vs Restante)  \n",
      "2   OvR_Class_3 (vs Restante)  \n",
      "3   OvR_Class_4 (vs Restante)  \n",
      "4   OvR_Class_5 (vs Restante)  \n",
      "\n",
      "Resultados consolidados salvos em: C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data\\cnn_ovr_results.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data'\n",
    "\n",
    "INPUT_FILES = [\n",
    "    'features_z_score.csv',\n",
    "    'features_min_max_0_1.csv',\n",
    "    'features_signal_norm_-1_1_z_score.csv',\n",
    "    'features_signal_norm_-1_1_min_max_0_1.csv'\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "KFOLD_SPLITS = 5 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_BINARY_CLASSES = 2 \n",
    "NUM_CLASSES = 5\n",
    "TARGET_CLASSES = [1, 2, 3, 4, 5] \n",
    "\n",
    "# ARQUITETURA DA CNN\n",
    "\n",
    "class BinaryCNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_output_classes: int = 2):\n",
    "        super(BinaryCNN, self).__init__()\n",
    "        \n",
    "        # Bloco Convolucional (5 Camadas)\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), \n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # O cálculo do tamanho da camada linear depende da dimensão de entrada\n",
    "        # Garante que a dimensão seja divisível por 32\n",
    "        input_dim_check = input_dim // 32\n",
    "        if input_dim_check == 0:\n",
    "            # Caso a dimensão seja muito pequena, use 1 como fallback ou ajuste o modelo\n",
    "            print(f\"AVISO: Dimensão de entrada ({input_dim}) muito pequena. Usando 1.\")\n",
    "            input_dim_check = 1 \n",
    "            \n",
    "        flatten_size = 512 * input_dim_check\n",
    "        \n",
    "        # Bloco Densely Connected\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(flatten_size, 1024), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), \n",
    "            nn.Linear(1024, num_output_classes) # Sempre 2 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "# DATASET\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, is_ovr_target: bool = False):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        \n",
    "        if is_ovr_target:\n",
    "            self.y = torch.tensor(y, dtype=torch.long)\n",
    "        else:\n",
    "            # Rótulos originais (1-5)\n",
    "            self.y = torch.tensor(y, dtype=torch.long) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].unsqueeze(0), self.y[idx]\n",
    "\n",
    "# FUNÇÃO AUXILIAR DE TREINAMENTO BINÁRIO (OvR) E COLETA DE SCORES\n",
    "\n",
    "def train_ovr_model_and_collect_scores(\n",
    "    X: np.ndarray, \n",
    "    y_binary: np.ndarray, \n",
    "    input_dim: int, \n",
    "    target_class: int,\n",
    "    file_fold_seed: int\n",
    ") -> Tuple[Dict[str, float], List[Tuple[np.ndarray, np.ndarray]]]:\n",
    "    \"\"\"Treina e avalia OvR, retornando métricas e scores de validação.\"\"\"\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=42 + file_fold_seed) \n",
    "    fold_metrics = {'accuracy': [], 'f1_weighted': []}\n",
    "    \n",
    "    # Armazenará (indices de validação, scores de validação) para agregação\n",
    "    all_validation_data = [] \n",
    "\n",
    "    for k_fold, (train_index, val_index) in enumerate(skf.split(X, y_binary)):\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y_binary[train_index], y_binary[val_index]\n",
    "\n",
    "        # Datasets com rótulos binários (OvR)\n",
    "        train_dataset = FeatureDataset(X_train, y_train, is_ovr_target=True)\n",
    "        val_dataset = FeatureDataset(X_val, y_val, is_ovr_target=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Inicialização e Treinamento do Modelo\n",
    "        model = BinaryCNN(input_dim, NUM_BINARY_CLASSES).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            model.train()\n",
    "            for features, labels in train_loader:\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Avaliação e Coleta de Scores\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "        fold_labels = []\n",
    "        fold_scores = [] # Logits brutos\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(features)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                fold_preds.extend(predicted.cpu().numpy())\n",
    "                fold_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Coletar os logits (scores) para a classe positiva (índice 1)\n",
    "                fold_scores.extend(outputs[:, 1].cpu().numpy()) \n",
    "\n",
    "        # Métricas do Fold\n",
    "        acc = accuracy_score(fold_labels, fold_preds)\n",
    "        f1_w = f1_score(fold_labels, fold_preds, average='weighted', zero_division=0) \n",
    "        \n",
    "        fold_metrics['accuracy'].append(acc)\n",
    "        fold_metrics['f1_weighted'].append(f1_w)\n",
    "        \n",
    "        # Armazena os resultados para a agregação OvR\n",
    "        all_validation_data.append((val_index, np.array(fold_scores)))\n",
    "    \n",
    "    mean_metrics = {\n",
    "        'Target_Class': target_class,\n",
    "        'Mean_Accuracy': np.mean(fold_metrics['accuracy']),\n",
    "        'Std_Accuracy': np.std(fold_metrics['accuracy']),\n",
    "        'Mean_F1_Weighted': np.mean(fold_metrics['f1_weighted']),\n",
    "        'Std_F1_Weighted': np.std(fold_metrics['f1_weighted'])\n",
    "    }\n",
    "    \n",
    "    return mean_metrics, all_validation_data\n",
    "\n",
    "# FUNÇÃO PRINCIPAL DE TREINAMENTO OVR E AGREGAÇÃO PARA 5X5\n",
    "\n",
    "def run_ovr_training(df: pd.DataFrame, filename: str, file_index: int) -> Tuple[List[Dict], np.ndarray]:\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\nIniciando Pipeline One-vs-Rest (OvR) para: {filename}\\n{'='*70}\")\n",
    "\n",
    "    # Pré-processamento inicial\n",
    "    sex_mapping = {'M': 0, 'F': 1}\n",
    "    df['Sex'] = df['Sex'].map(sex_mapping)\n",
    "    \n",
    "    X_raw = df.drop(columns=['ID', 'Class']).values\n",
    "    y_original = df['Class'].values\n",
    "    \n",
    "    # Lidar com NaNs nos dados\n",
    "    X = np.nan_to_num(X_raw, nan=0.0) \n",
    "    INPUT_DIM = X.shape[1]\n",
    "    TOTAL_SAMPLES = len(X)\n",
    "    \n",
    "    print(f\"  Dimensão de Features (X): {X.shape}\")\n",
    "    \n",
    "    all_ovr_results = []\n",
    "    \n",
    "    # Inicializa a matriz para armazenar os scores de OvR para cada amostra (Total x Classes)\n",
    "    aggregated_ovr_scores = np.zeros((TOTAL_SAMPLES, NUM_CLASSES)) \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # Loop de Treinamento OvR para cada classe (1, 2, 3, 4, 5)\n",
    "    # ----------------------------------------------------\n",
    "    for target_class in TARGET_CLASSES:\n",
    "        level_name = f\"OvR_Class_{target_class} (vs Restante)\"\n",
    "        print(f\"\\n--- {level_name} ---\")\n",
    "        \n",
    "        # 1. Cria o target binário para o modelo OvR\n",
    "        y_binary = np.where(y_original == target_class, 1, 0)\n",
    "        \n",
    "        # 2. Treinamento e Coleta de Scores\n",
    "        results, validation_data = train_ovr_model_and_collect_scores(\n",
    "            X, y_binary, INPUT_DIM, target_class, file_fold_seed=file_index + target_class\n",
    "        )\n",
    "        \n",
    "        # 3. Consolida e armazena os resultados\n",
    "        results['Filename'] = filename\n",
    "        results['Classifier'] = level_name\n",
    "        all_ovr_results.append(results)\n",
    "        \n",
    "        print(f\"  Accuracy: {results['Mean_Accuracy']:.4f}, F1-W: {results['Mean_F1_Weighted']:.4f}\")\n",
    "\n",
    "        # 4. Agregação dos scores OvR\n",
    "        ovr_class_index = target_class - 1 \n",
    "        for val_index, scores in validation_data:\n",
    "            aggregated_ovr_scores[val_index, ovr_class_index] = scores\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # AGREGAÇÃO FINAL (5X5) PARA MATRIZ DE CONFUSÃO\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    # A previsão final é a classe cujo modelo OvR retornou o maior score (logit)\n",
    "    # Argmax retorna o índice (0 a 4), adicionamos 1 para a classe (1 a 5)\n",
    "    final_predictions_1_to_5 = np.argmax(aggregated_ovr_scores, axis=1) + 1 \n",
    "    \n",
    "    # Rótulos verdadeiros (1 a 5)\n",
    "    true_labels_1_to_5 = y_original\n",
    "    \n",
    "    # Calcula a Matriz de Confusão 5x5\n",
    "    conf_matrix = confusion_matrix(\n",
    "        true_labels_1_to_5, \n",
    "        final_predictions_1_to_5, \n",
    "        labels=TARGET_CLASSES\n",
    "    )\n",
    "    \n",
    "    return all_ovr_results, conf_matrix\n",
    "\n",
    "# FUNÇÃO DE EXIBIÇÃO DA MATRIZ\n",
    "\n",
    "def display_confusion_matrix(conf_matrix: np.ndarray, filename: str, save_dir: str = OUTPUT_DIR):\n",
    "    \"\"\"Formata, imprime e salva a matriz de confusão (5x5).\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(f\"MATRIZ DE CONFUSÃO (5x5) OVR - Agregação de K-Fold para: {filename}\")\n",
    "    print(f\"Rótulos: [1, 2, 3, 4, 5]\")\n",
    "    print(\"#\"*70)\n",
    "    \n",
    "    header = [\"Classe Real ->\"] + [f\"Pred {c}\" for c in TARGET_CLASSES]\n",
    "    print(\"{:<15}\".format(\"\"), end=\"\")\n",
    "    print(\" \".join([\"{:>8}\".format(h) for h in header[1:]]))\n",
    "    print(\"-\" * (15 + 8 * 5))\n",
    "    for i in range(5):\n",
    "        row_label = f\"Classe {TARGET_CLASSES[i]} |\"\n",
    "        print(\"{:<15}\".format(row_label), end=\"\")\n",
    "        for j in range(5):\n",
    "            print(\"{:>8}\".format(conf_matrix[i, j]), end=\"\")\n",
    "        print()\n",
    "    print(\"#\"*70)\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=TARGET_CLASSES, yticklabels=TARGET_CLASSES)\n",
    "    plt.title(f'Matriz de Confusão - {filename}')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f'confusion_matrix_{filename.replace(\".csv\", \"\")}.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Matriz de confusão salva em: {save_path}\")\n",
    " \n",
    "# EXECUÇÃO\n",
    "missing_files = [f for f in INPUT_FILES if not os.path.exists(os.path.join(OUTPUT_DIR, f))]\n",
    "if missing_files:\n",
    "    print(f\"AVISO: Arquivos não encontrados no diretório {OUTPUT_DIR}. O script tentará pular os ausentes:\")\n",
    "    for f in missing_files:\n",
    "        print(f\"- {f}\")\n",
    "\n",
    "all_experiment_results = []\n",
    "first_conf_matrix = None\n",
    "first_filename = None\n",
    "\n",
    "for idx, filename in enumerate(INPUT_FILES):\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # O run_ovr_training retorna a lista de resultados OvR e a matriz 5x5 agregada\n",
    "        results_list, conf_matrix = run_ovr_training(df, filename, file_index=idx)\n",
    "        \n",
    "        all_experiment_results.extend(results_list)\n",
    "        \n",
    "        display_confusion_matrix(conf_matrix, filename, save_dir=OUTPUT_DIR)\n",
    "        \n",
    "        # Armazena a primeira matriz para exibição detalhada\n",
    "        if first_conf_matrix is None:\n",
    "            first_conf_matrix = conf_matrix\n",
    "            first_filename = filename\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o arquivo {filename}: {e}\", file=sys.stderr)\n",
    "\n",
    "if first_conf_matrix is not None:\n",
    "    display_confusion_matrix(first_conf_matrix, first_filename, save_dir=OUTPUT_DIR)\n",
    "\n",
    "results_df = pd.DataFrame(all_experiment_results)\n",
    "if not results_df.empty:\n",
    "    output_results_path = os.path.join(OUTPUT_DIR, 'cnn_ovr_results.csv')\n",
    "    results_df.to_csv(output_results_path, index=False)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"RESUMO FINAL DOS EXPERIMENTOS ONE-VS-REST (OvR)\")\n",
    "    print(\"=\"*70)\n",
    "    # Exibir o resumo agrupado por arquivo e classificador\n",
    "    print(results_df.sort_values(by=['Filename', 'Target_Class']))\n",
    "    print(f\"\\nResultados consolidados salvos em: {output_results_path}\")\n",
    "else:\n",
    "    print(\"\\nNenhum resultado foi gerado. Verifique se os arquivos de entrada existem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a51a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INICIANDO SELEÇÃO SEQUENCIAL DE FEATURES (SFS) para: features_z_score.csv\n",
      "======================================================================\n",
      "1. Teste Inicial (Base): ['Age', 'Sex', 'Fo_mean_Hz_phonationA', 'Fo_mean_Hz_phonationE', 'Fo_mean_Hz_phonationI', 'Fo_mean_Hz_phonationO', 'Fo_mean_Hz_phonationU'] (7 colunas)\n",
      "-> F1-Weighted Inicial: 0.2662\n",
      "\n",
      "SFS FORWARD\n",
      "Step 1 - Testando grupo 'Fhi': F1=0.3260 (Ganho: 0.0598)\n",
      "Step 1 - Testando grupo 'Flo': F1=0.3449 (Ganho: 0.0788)\n",
      "Step 1 - Testando grupo 'F0': F1=0.2993 (Ganho: 0.0331)\n",
      "Step 1 - Testando grupo 'Jitter': F1=0.3385 (Ganho: 0.0723)\n",
      "Step 1 - Testando grupo 'Jitter': F1=0.3017 (Ganho: 0.0356)\n",
      "Step 1 - Testando grupo 'RAP': F1=0.3136 (Ganho: 0.0474)\n",
      "Step 1 - Testando grupo 'PPQ': F1=0.3023 (Ganho: 0.0362)\n",
      "Step 1 - Testando grupo 'DDP': F1=0.3114 (Ganho: 0.0452)\n",
      "Step 1 - Testando grupo 'Shimmer': F1=0.3478 (Ganho: 0.0817)\n",
      "Step 1 - Testando grupo 'Shimmer': F1=0.3258 (Ganho: 0.0597)\n",
      "Step 1 - Testando grupo 'Shimmer': F1=0.3447 (Ganho: 0.0785)\n",
      "Step 1 - Testando grupo 'Shimmer': F1=0.3502 (Ganho: 0.0841)\n",
      "Step 1 - Testando grupo 'Shimmer': F1=0.3493 (Ganho: 0.0831)\n",
      "Step 1 - Testando grupo 'Shimmer': F1=0.3294 (Ganho: 0.0633)\n",
      "Step 1 - Testando grupo 'NHR': F1=0.3353 (Ganho: 0.0691)\n",
      "Step 1 - Testando grupo 'HNR': F1=0.3573 (Ganho: 0.0912)\n",
      "Step 1 - Testando grupo 'RPDE': F1=0.3387 (Ganho: 0.0725)\n",
      "Step 1 - Testando grupo 'DFA': F1=0.3324 (Ganho: 0.0662)\n",
      "Step 1 - Testando grupo 'spread1': F1=0.3212 (Ganho: 0.0550)\n",
      "Step 1 - Testando grupo 'spread2': F1=0.2924 (Ganho: 0.0262)\n",
      "Step 1 - Testando grupo 'D2': F1=0.3302 (Ganho: 0.0641)\n",
      "Step 1 - Testando grupo 'PPE': F1=0.3709 (Ganho: 0.1048)\n",
      "\n",
      "Passo 1 (ADD): Grupo 'PPE' adicionado.\n",
      "-> Novo Melhor F1-Weighted: 0.3709 (12 colunas)\n",
      "\n",
      "Step 2 - Testando grupo 'Fhi': F1=0.3687 (Ganho: -0.0022)\n",
      "Step 2 - Testando grupo 'Flo': F1=0.3634 (Ganho: -0.0075)\n",
      "Step 2 - Testando grupo 'F0': F1=0.3568 (Ganho: -0.0141)\n",
      "Step 2 - Testando grupo 'Jitter': F1=0.3268 (Ganho: -0.0441)\n",
      "Step 2 - Testando grupo 'Jitter': F1=0.3658 (Ganho: -0.0051)\n",
      "Step 2 - Testando grupo 'RAP': F1=0.3652 (Ganho: -0.0057)\n",
      "Step 2 - Testando grupo 'PPQ': F1=0.3217 (Ganho: -0.0492)\n",
      "Step 2 - Testando grupo 'DDP': F1=0.3396 (Ganho: -0.0313)\n",
      "Step 2 - Testando grupo 'Shimmer': F1=0.3558 (Ganho: -0.0151)\n",
      "Step 2 - Testando grupo 'Shimmer': F1=0.3580 (Ganho: -0.0129)\n",
      "Step 2 - Testando grupo 'Shimmer': F1=0.3308 (Ganho: -0.0402)\n",
      "Step 2 - Testando grupo 'Shimmer': F1=0.3925 (Ganho: 0.0216)\n",
      "Step 2 - Testando grupo 'Shimmer': F1=0.3829 (Ganho: 0.0120)\n",
      "Step 2 - Testando grupo 'Shimmer': F1=0.3335 (Ganho: -0.0374)\n",
      "Step 2 - Testando grupo 'NHR': F1=0.3908 (Ganho: 0.0199)\n",
      "Step 2 - Testando grupo 'HNR': F1=0.3737 (Ganho: 0.0028)\n",
      "Step 2 - Testando grupo 'RPDE': F1=0.3845 (Ganho: 0.0135)\n",
      "Step 2 - Testando grupo 'DFA': F1=0.3952 (Ganho: 0.0242)\n",
      "Step 2 - Testando grupo 'spread1': F1=0.4183 (Ganho: 0.0474)\n",
      "Step 2 - Testando grupo 'spread2': F1=0.3829 (Ganho: 0.0120)\n",
      "Step 2 - Testando grupo 'D2': F1=0.3604 (Ganho: -0.0106)\n",
      "\n",
      "Passo 2 (ADD): Grupo 'spread1' adicionado.\n",
      "-> Novo Melhor F1-Weighted: 0.4183 (17 colunas)\n",
      "\n",
      "Step 3 - Testando grupo 'Fhi': F1=0.3608 (Ganho: -0.0575)\n",
      "Step 3 - Testando grupo 'Flo': F1=0.3839 (Ganho: -0.0344)\n",
      "Step 3 - Testando grupo 'F0': F1=0.3610 (Ganho: -0.0573)\n",
      "Step 3 - Testando grupo 'Jitter': F1=0.4306 (Ganho: 0.0122)\n",
      "Step 3 - Testando grupo 'Jitter': F1=0.4248 (Ganho: 0.0065)\n",
      "Step 3 - Testando grupo 'RAP': F1=0.3970 (Ganho: -0.0213)\n",
      "Step 3 - Testando grupo 'PPQ': F1=0.4075 (Ganho: -0.0108)\n",
      "Step 3 - Testando grupo 'DDP': F1=0.4159 (Ganho: -0.0024)\n",
      "Step 3 - Testando grupo 'Shimmer': F1=0.3723 (Ganho: -0.0460)\n",
      "Step 3 - Testando grupo 'Shimmer': F1=0.4010 (Ganho: -0.0173)\n",
      "Step 3 - Testando grupo 'Shimmer': F1=0.3900 (Ganho: -0.0283)\n",
      "Step 3 - Testando grupo 'Shimmer': F1=0.3981 (Ganho: -0.0202)\n",
      "Step 3 - Testando grupo 'Shimmer': F1=0.4212 (Ganho: 0.0028)\n",
      "Step 3 - Testando grupo 'Shimmer': F1=0.4084 (Ganho: -0.0099)\n",
      "Step 3 - Testando grupo 'NHR': F1=0.4293 (Ganho: 0.0109)\n",
      "Step 3 - Testando grupo 'HNR': F1=0.3890 (Ganho: -0.0293)\n",
      "Step 3 - Testando grupo 'RPDE': F1=0.3995 (Ganho: -0.0188)\n",
      "Step 3 - Testando grupo 'DFA': F1=0.4004 (Ganho: -0.0180)\n",
      "Step 3 - Testando grupo 'spread2': F1=0.3802 (Ganho: -0.0381)\n",
      "Step 3 - Testando grupo 'D2': F1=0.4013 (Ganho: -0.0170)\n",
      "\n",
      "Passo 3 (ADD): Grupo 'Jitter' adicionado.\n",
      "-> Novo Melhor F1-Weighted: 0.4306 (22 colunas)\n",
      "\n",
      "Step 4 - Testando grupo 'Fhi': F1=0.3605 (Ganho: -0.0701)\n",
      "Step 4 - Testando grupo 'Flo': F1=0.3975 (Ganho: -0.0330)\n",
      "Step 4 - Testando grupo 'F0': F1=0.3604 (Ganho: -0.0702)\n",
      "Step 4 - Testando grupo 'Jitter': F1=0.4036 (Ganho: -0.0270)\n",
      "Step 4 - Testando grupo 'RAP': F1=0.3586 (Ganho: -0.0719)\n",
      "Step 4 - Testando grupo 'PPQ': F1=0.3997 (Ganho: -0.0309)\n",
      "Step 4 - Testando grupo 'DDP': F1=0.4096 (Ganho: -0.0209)\n",
      "Step 4 - Testando grupo 'Shimmer': F1=0.3850 (Ganho: -0.0456)\n",
      "Step 4 - Testando grupo 'Shimmer': F1=0.4208 (Ganho: -0.0098)\n",
      "Step 4 - Testando grupo 'Shimmer': F1=0.3983 (Ganho: -0.0322)\n",
      "Step 4 - Testando grupo 'Shimmer': F1=0.3755 (Ganho: -0.0550)\n",
      "Step 4 - Testando grupo 'Shimmer': F1=0.3726 (Ganho: -0.0580)\n",
      "Step 4 - Testando grupo 'Shimmer': F1=0.3955 (Ganho: -0.0350)\n",
      "Step 4 - Testando grupo 'NHR': F1=0.4372 (Ganho: 0.0066)\n",
      "Step 4 - Testando grupo 'HNR': F1=0.3887 (Ganho: -0.0419)\n",
      "Step 4 - Testando grupo 'RPDE': F1=0.3869 (Ganho: -0.0437)\n",
      "Step 4 - Testando grupo 'DFA': F1=0.3846 (Ganho: -0.0460)\n",
      "Step 4 - Testando grupo 'spread2': F1=0.4240 (Ganho: -0.0066)\n",
      "Step 4 - Testando grupo 'D2': F1=0.3929 (Ganho: -0.0377)\n",
      "\n",
      "Passo 4 (ADD): Grupo 'NHR' adicionado.\n",
      "-> Novo Melhor F1-Weighted: 0.4372 (27 colunas)\n",
      "\n",
      "Step 5 - Testando grupo 'Fhi': F1=0.3943 (Ganho: -0.0428)\n",
      "Step 5 - Testando grupo 'Flo': F1=0.3648 (Ganho: -0.0724)\n",
      "Step 5 - Testando grupo 'F0': F1=0.3791 (Ganho: -0.0581)\n",
      "Step 5 - Testando grupo 'Jitter': F1=0.4461 (Ganho: 0.0090)\n",
      "Step 5 - Testando grupo 'RAP': F1=0.3743 (Ganho: -0.0628)\n",
      "Step 5 - Testando grupo 'PPQ': F1=0.4093 (Ganho: -0.0279)\n",
      "Step 5 - Testando grupo 'DDP': F1=0.3748 (Ganho: -0.0624)\n",
      "Step 5 - Testando grupo 'Shimmer': F1=0.3959 (Ganho: -0.0413)\n",
      "Step 5 - Testando grupo 'Shimmer': F1=0.3709 (Ganho: -0.0663)\n",
      "Step 5 - Testando grupo 'Shimmer': F1=0.3991 (Ganho: -0.0381)\n",
      "Step 5 - Testando grupo 'Shimmer': F1=0.3845 (Ganho: -0.0527)\n",
      "Step 5 - Testando grupo 'Shimmer': F1=0.3674 (Ganho: -0.0697)\n",
      "Step 5 - Testando grupo 'Shimmer': F1=0.3935 (Ganho: -0.0437)\n",
      "Step 5 - Testando grupo 'HNR': F1=0.4125 (Ganho: -0.0247)\n",
      "Step 5 - Testando grupo 'RPDE': F1=0.4159 (Ganho: -0.0213)\n",
      "Step 5 - Testando grupo 'DFA': F1=0.3941 (Ganho: -0.0431)\n",
      "Step 5 - Testando grupo 'spread2': F1=0.4194 (Ganho: -0.0178)\n",
      "Step 5 - Testando grupo 'D2': F1=0.3774 (Ganho: -0.0598)\n",
      "\n",
      "Passo 5 (ADD): Grupo 'Jitter' adicionado.\n",
      "-> Novo Melhor F1-Weighted: 0.4461 (32 colunas)\n",
      "\n",
      "Step 6 - Testando grupo 'Fhi': F1=0.3572 (Ganho: -0.0890)\n",
      "Step 6 - Testando grupo 'Flo': F1=0.3897 (Ganho: -0.0564)\n",
      "Step 6 - Testando grupo 'F0': F1=0.3414 (Ganho: -0.1047)\n",
      "Step 6 - Testando grupo 'RAP': F1=0.3702 (Ganho: -0.0759)\n",
      "Step 6 - Testando grupo 'PPQ': F1=0.3947 (Ganho: -0.0515)\n",
      "Step 6 - Testando grupo 'DDP': F1=0.3864 (Ganho: -0.0598)\n",
      "Step 6 - Testando grupo 'Shimmer': F1=0.3614 (Ganho: -0.0847)\n",
      "Step 6 - Testando grupo 'Shimmer': F1=0.4017 (Ganho: -0.0444)\n",
      "Step 6 - Testando grupo 'Shimmer': F1=0.3975 (Ganho: -0.0487)\n",
      "Step 6 - Testando grupo 'Shimmer': F1=0.3916 (Ganho: -0.0545)\n",
      "Step 6 - Testando grupo 'Shimmer': F1=0.3768 (Ganho: -0.0693)\n",
      "Step 6 - Testando grupo 'Shimmer': F1=0.4335 (Ganho: -0.0127)\n",
      "Step 6 - Testando grupo 'HNR': F1=0.3704 (Ganho: -0.0757)\n",
      "Step 6 - Testando grupo 'RPDE': F1=0.4062 (Ganho: -0.0399)\n",
      "Step 6 - Testando grupo 'DFA': F1=0.3796 (Ganho: -0.0665)\n",
      "Step 6 - Testando grupo 'spread2': F1=0.3670 (Ganho: -0.0791)\n",
      "Step 6 - Testando grupo 'D2': F1=0.4072 (Ganho: -0.0389)\n",
      "\n",
      "Passo 6 (STOP): Nenhuma melhoria significativa (-0.0127). Fim do SFS Forward.\n",
      "\n",
      "======================================================================\n",
      "RESULTADO FINAL DO SFS PARA features_z_score.csv\n",
      "Melhor Conjunto de Features (32): ['Age', 'Sex', 'Fo_mean_Hz_phonationA', 'Fo_mean_Hz_phonationE', 'Fo_mean_Hz_phonationI', 'Fo_mean_Hz_phonationO', 'Fo_mean_Hz_phonationU', 'PPE_phonationA', 'PPE_phonationE', 'PPE_phonationI', 'PPE_phonationO', 'PPE_phonationU', 'spread1_phonationA', 'spread1_phonationE', 'spread1_phonationI', 'spread1_phonationO', 'spread1_phonationU', 'Jitter_percent_phonationA', 'Jitter_percent_phonationE', 'Jitter_percent_phonationI', 'Jitter_percent_phonationO', 'Jitter_percent_phonationU', 'NHR_phonationA', 'NHR_phonationE', 'NHR_phonationI', 'NHR_phonationO', 'NHR_phonationU', 'Jitter_Abs_phonationA', 'Jitter_Abs_phonationE', 'Jitter_Abs_phonationI', 'Jitter_Abs_phonationO', 'Jitter_Abs_phonationU']\n",
      "Melhor F1-Weighted Agregado: 0.4461\n",
      "======================================================================\n",
      "\n",
      "######################################################################\n",
      "MATRIZ DE CONFUSÃO (5x5) OVR - Agregação de K-Fold para: SFS_Final_features_z_score.csv\n",
      "Rótulos: [1, 2, 3, 4, 5]\n",
      "######################################################################\n",
      "                 Pred 1   Pred 2   Pred 3   Pred 4   Pred 5\n",
      "-------------------------------------------------------\n",
      "Classe 1 |            1       1       1       2       1\n",
      "Classe 2 |            0       7       7       6       6\n",
      "Classe 3 |            0       8      10      16      23\n",
      "Classe 4 |            0       5      11      31      29\n",
      "Classe 5 |            0       3      21      28      55\n",
      "######################################################################\n",
      "Matriz de confusão salva em: C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data\\confusion_matrix_SFS_Final_features_z_score.png\n",
      "Histórico do SFS salvo em: C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data\\sfs_history_features_z_score.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\data'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_FILE_FOR_SFS = 'features_z_score.csv'\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "KFOLD_SPLITS = 5 \n",
    "NUM_BINARY_CLASSES = 2 \n",
    "NUM_CLASSES = 5\n",
    "TARGET_CLASSES = [1, 2, 3, 4, 5] \n",
    "\n",
    "ALL_COLUMNS = [\n",
    "    'Age', 'Sex',\n",
    "    'Fo_mean_Hz', 'Fhi_max_Hz', 'Flo_min_Hz', 'F0_std_Hz', \n",
    "    'Jitter_percent', 'Jitter_Abs', 'RAP', 'PPQ', 'DDP', \n",
    "    'Shimmer_local', 'Shimmer_dB', 'Shimmer_APQ3', 'Shimmer_APQ5', \n",
    "    'Shimmer_APQ11', 'Shimmer_DDA', 'NHR', 'HNR', 'RPDE', \n",
    "    'DFA', 'spread1', 'spread2', 'D2', 'PPE'\n",
    "]\n",
    "\n",
    "def get_grouped_columns(base_col: str) -> List[str]:\n",
    "    \"\"\"Expande uma feature base para incluir todos os grupos (A, E, I, O, U, KA, PA, TA).\"\"\"\n",
    "    if base_col in ['Age', 'Sex']:\n",
    "        return [base_col]\n",
    "    \n",
    "    # Grupos de vogais (A, E, I, O, U)\n",
    "    vowels = ['phonationA', 'phonationE', 'phonationI', 'phonationO', 'phonationU']\n",
    "    vowel_cols = [f\"{base_col}_{v}\" for v in vowels]\n",
    "    \n",
    "    # # Grupos de ritmos (KA, PA, TA)\n",
    "    # rhythms = ['rhythmKA', 'rhythmPA', 'rhythmTA']\n",
    "    # rhythm_cols = [f\"{base_col}_{r}\" for r in rhythms]\n",
    "    \n",
    "    return vowel_cols\n",
    "\n",
    "# Define a ordem de adição dos grupos de features e remove os grupos que já estão no início do fluxo para evitar re-adição.\n",
    "FEATURE_GROUPS = [get_grouped_columns(col) for col in ALL_COLUMNS]\n",
    "FEATURE_GROUPS = [g for g in FEATURE_GROUPS if g[0] not in ['Age', 'Sex', 'Fo_mean_Hz_phonationA', 'Fo_mean_Hz_phonationE', 'Fo_mean_Hz_phonationI', 'Fo_mean_Hz_phonationO', 'Fo_mean_Hz_phonationU']] \n",
    "\n",
    "# DATASET\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, is_ovr_target: bool = False):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].unsqueeze(0), self.y[idx]\n",
    "\n",
    "# CNN\n",
    "class BinaryCNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_output_classes: int = 2):\n",
    "        super(BinaryCNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        input_dim_after_conv = input_dim // (2**2) \n",
    "        \n",
    "        if input_dim_after_conv == 0:\n",
    "             input_dim_after_conv = 1 \n",
    "             \n",
    "        flatten_size = 64 * input_dim_after_conv\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(flatten_size, 1024), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), \n",
    "            nn.Linear(1024, num_output_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "# TREINAMENTO\n",
    "def train_ovr_model_and_collect_scores(X: np.ndarray, y_binary: np.ndarray, input_dim: int, target_class: int, file_fold_seed: int) -> Tuple[Dict[str, float], List[Tuple[np.ndarray, np.ndarray]]]:\n",
    "    skf = StratifiedKFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=42 + file_fold_seed) \n",
    "    fold_metrics = {'accuracy': [], 'f1_weighted': []}\n",
    "    all_validation_data = [] \n",
    "    \n",
    "    for k_fold, (train_index, val_index) in enumerate(skf.split(X, y_binary)):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y_binary[train_index], y_binary[val_index]\n",
    "\n",
    "        train_dataset = FeatureDataset(X_train, y_train, is_ovr_target=True)\n",
    "        val_dataset = FeatureDataset(X_val, y_val, is_ovr_target=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = BinaryCNN(input_dim, NUM_BINARY_CLASSES).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            model.train()\n",
    "            for features, labels in train_loader:\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "        fold_labels = []\n",
    "        fold_scores = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(features)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                fold_preds.extend(predicted.cpu().numpy())\n",
    "                fold_labels.extend(labels.cpu().numpy())\n",
    "                fold_scores.extend(outputs[:, 1].cpu().numpy()) \n",
    "\n",
    "        acc = accuracy_score(fold_labels, fold_preds)\n",
    "        f1_w = f1_score(fold_labels, fold_preds, average='weighted', zero_division=0) \n",
    "        \n",
    "        fold_metrics['accuracy'].append(acc)\n",
    "        fold_metrics['f1_weighted'].append(f1_w)\n",
    "        all_validation_data.append((val_index, np.array(fold_scores)))\n",
    "    \n",
    "    mean_f1_weighted = np.mean(fold_metrics['f1_weighted'])\n",
    "    \n",
    "    mean_metrics = {\n",
    "        'Target_Class': target_class,\n",
    "        'Mean_Accuracy': np.mean(fold_metrics['accuracy']),\n",
    "        'Std_Accuracy': np.std(fold_metrics['accuracy']),\n",
    "        'Mean_F1_Weighted': mean_f1_weighted,\n",
    "        'Std_F1_Weighted': np.std(fold_metrics['f1_weighted'])\n",
    "    }\n",
    "    \n",
    "    return mean_metrics, all_validation_data\n",
    "\n",
    "def run_ovr_training(df: pd.DataFrame, filename: str, file_index: int, feature_cols: List[str]) -> Tuple[float, np.ndarray]:\n",
    "    sex_mapping = {'M': 0, 'F': 1}\n",
    "    df_temp = df.copy()\n",
    "    df_temp['Sex'] = df_temp['Sex'].map(sex_mapping)\n",
    "    \n",
    "    X_raw = df_temp[feature_cols].values \n",
    "    y_original = df_temp['Class'].values\n",
    "    \n",
    "    X = np.nan_to_num(X_raw, nan=0.0) \n",
    "    INPUT_DIM = X.shape[1]\n",
    "    TOTAL_SAMPLES = len(X)\n",
    "    \n",
    "    aggregated_ovr_scores = np.zeros((TOTAL_SAMPLES, NUM_CLASSES)) \n",
    "    \n",
    "    for target_class in TARGET_CLASSES:\n",
    "        y_binary = np.where(y_original == target_class, 1, 0)\n",
    "        results, validation_data = train_ovr_model_and_collect_scores(\n",
    "            X, y_binary, INPUT_DIM, target_class, file_fold_seed=file_index + target_class\n",
    "        )\n",
    "\n",
    "        ovr_class_index = target_class - 1 \n",
    "        for val_index, scores in validation_data:\n",
    "            aggregated_ovr_scores[val_index, ovr_class_index] = scores\n",
    "\n",
    "    final_predictions_1_to_5 = np.argmax(aggregated_ovr_scores, axis=1) + 1 \n",
    "    true_labels_1_to_5 = y_original\n",
    "    \n",
    "    conf_matrix = confusion_matrix(\n",
    "        true_labels_1_to_5, \n",
    "        final_predictions_1_to_5, \n",
    "        labels=TARGET_CLASSES\n",
    "    )\n",
    "    \n",
    "    total_f1_weighted = f1_score(true_labels_1_to_5, final_predictions_1_to_5, average='weighted', zero_division=0)\n",
    "    \n",
    "    return total_f1_weighted, conf_matrix\n",
    "\n",
    "# MATRIZ DE CONFUSÃO\n",
    "def display_confusion_matrix(conf_matrix: np.ndarray, filename: str, save_dir: str = OUTPUT_DIR):\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(f\"MATRIZ DE CONFUSÃO (5x5) OVR - Agregação de K-Fold para: {filename}\")\n",
    "    print(f\"Rótulos: [1, 2, 3, 4, 5]\")\n",
    "    print(\"#\"*70)\n",
    "    \n",
    "    header = [\"Classe Real ->\"] + [f\"Pred {c}\" for c in TARGET_CLASSES]\n",
    "    print(\"{:<15}\".format(\"\"), end=\"\")\n",
    "    print(\" \".join([\"{:>8}\".format(h) for h in header[1:]]))\n",
    "    print(\"-\" * (15 + 8 * 5))\n",
    "    for i in range(5):\n",
    "        row_label = f\"Classe {TARGET_CLASSES[i]} |\"\n",
    "        print(\"{:<15}\".format(row_label), end=\"\")\n",
    "        for j in range(5):\n",
    "            print(\"{:>8}\".format(conf_matrix[i, j]), end=\"\")\n",
    "        print()\n",
    "    print(\"#\"*70)\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=TARGET_CLASSES, yticklabels=TARGET_CLASSES)\n",
    "        plt.title(f'Matriz de Confusão - {filename}')\n",
    "        plt.xlabel('Predito')\n",
    "        plt.ylabel('Real')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        save_path = os.path.join(save_dir, f'confusion_matrix_{filename.replace(\".csv\", \"\")}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Matriz de confusão salva em: {save_path}\")\n",
    "    except Exception as e:\n",
    "         print(f\"AVISO: Não foi possível salvar a matriz de confusão gráfica. Erro: {e}\")\n",
    "\n",
    "# SELEÇÃO SEQUENCIAL DE FEATURES (SFS Forward)\n",
    "def sequential_feature_selection(df: pd.DataFrame, filename: str, file_index: int):\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\nINICIANDO SELEÇÃO SEQUENCIAL DE FEATURES (SFS) para: {filename}\\n{'='*70}\")\n",
    "    \n",
    "    initial_cols = ['Age', 'Sex']\n",
    "    initial_cols.extend(get_grouped_columns('Fo_mean_Hz'))\n",
    "    \n",
    "    current_features = initial_cols \n",
    "    remaining_groups = FEATURE_GROUPS\n",
    "    \n",
    "    print(f\"Teste Inicial (Base): {current_features} ({len(current_features)} colunas)\")\n",
    "    best_score, best_conf_matrix = run_ovr_training(df, filename, file_index, current_features)\n",
    "    print(f\"F1-Weighted Inicial: {best_score:.4f}\")\n",
    "    \n",
    "    print(\"\\nSFS FORWARD\")\n",
    "    \n",
    "    history = [\n",
    "        {'step': 0, 'features': list(current_features), 'score': best_score, 'action': 'START'}\n",
    "    ]\n",
    "\n",
    "    step = 1\n",
    "    while remaining_groups:\n",
    "        \n",
    "        best_gain = -1.0\n",
    "        best_group_to_add = None\n",
    "        best_temp_score = best_score\n",
    "        \n",
    "        for group_to_add in remaining_groups:\n",
    "            temp_features = current_features + group_to_add\n",
    "            temp_score, _ = run_ovr_training(df, filename, file_index, temp_features)\n",
    "            gain = temp_score - best_score\n",
    "            group_name = group_to_add[0].split('_')[0]\n",
    "            print(f\"Step {step} - Testando grupo '{group_name}': F1={temp_score:.4f} (Ganho: {gain:.4f})\")\n",
    "            \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_group_to_add = group_to_add\n",
    "                best_temp_score = temp_score\n",
    "        \n",
    "        if best_gain > 1e-4:\n",
    "            current_features.extend(best_group_to_add)\n",
    "            best_score = best_temp_score\n",
    "            remaining_groups.remove(best_group_to_add)\n",
    "            _, best_conf_matrix = run_ovr_training(df, filename, file_index, current_features) \n",
    "            \n",
    "            group_name = best_group_to_add[0].split('_')[0]\n",
    "            print(f\"\\nPasso {step} (ADD): Grupo '{group_name}' adicionado.\")\n",
    "            print(f\"Novo Melhor F1-Weighted: {best_score:.4f} ({len(current_features)} colunas)\\n\")\n",
    "            \n",
    "            history.append({\n",
    "                'step': step, \n",
    "                'features': list(current_features), \n",
    "                'score': best_score, \n",
    "                'action': f'ADD: {group_name}'\n",
    "            })\n",
    "            \n",
    "            step += 1\n",
    "        else:\n",
    "            print(f\"\\nPasso {step} (STOP): Nenhuma melhoria significativa ({best_gain:.4f}). Fim do SFS Forward.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"RESULTADO FINAL DO SFS PARA {filename}\")\n",
    "    print(f\"Melhor Conjunto de Features ({len(current_features)}): {current_features}\")\n",
    "    print(f\"Melhor F1-Weighted Agregado: {best_score:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    display_confusion_matrix(best_conf_matrix, f\"SFS_Final_{filename}\", save_dir=OUTPUT_DIR)\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_path = os.path.join(OUTPUT_DIR, f'sfs_history_{filename.replace(\".csv\", \"\")}.csv')\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    print(f\"Histórico do SFS salvo em: {history_path}\")\n",
    "\n",
    "# EXECUÇÃO\n",
    "try:\n",
    "    filepath = os.path.join(OUTPUT_DIR, INPUT_FILE_FOR_SFS)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"ERRO: Arquivo '{INPUT_FILE_FOR_SFS}' não encontrado em '{OUTPUT_DIR}'\")\n",
    "    else:\n",
    "        df = pd.read_csv(filepath)\n",
    "        sequential_feature_selection(df, INPUT_FILE_FOR_SFS, file_index=0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro fatal ao carregar ou processar o arquivo {INPUT_FILE_FOR_SFS}: {e}\", file=sys.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
