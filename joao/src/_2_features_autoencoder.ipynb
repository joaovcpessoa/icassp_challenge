{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f34a43",
   "metadata": {},
   "source": [
    "##### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e742988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do Cuda: 2.9.0+cu130\n",
      "Disponibilidade da GPU: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f'Versão do Cuda: {torch.__version__}')\n",
    "print(f'Disponibilidade da GPU: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20911e60",
   "metadata": {},
   "source": [
    "##### Ajuste dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a70a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceFeatureDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=8, latent_dim=2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "CSV_PATH = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\joao\\data\\features_all.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "X = df.drop(columns=['ID', 'Age', 'Sex', 'Class']).values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "dataset = VoiceFeatureDataset(X_scaled)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a1b2f",
   "metadata": {},
   "source": [
    "##### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8de4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Autoencoder(input_dim=8, latent_dim=3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 100\n",
    "losses = []\n",
    "pseudo_accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for features in dataloader:\n",
    "        features = features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon, z = model(features)\n",
    "        loss = criterion(recon, features)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Pseudo-accuracy baseado no MSE\n",
    "        batch_acc = 1 - torch.mean((recon - features)**2).item()\n",
    "        total_acc += batch_acc\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_acc = total_acc / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    pseudo_accs.append(avg_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{n_epochs} | Loss: {avg_loss:.6f}  | Accuracy: {avg_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4508c",
   "metadata": {},
   "source": [
    "##### Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e882719",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(losses, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Loss\")\n",
    "plt.title(\"Loss por Época\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(pseudo_accs, marker='o', color='green')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Pseudo-Accuracy\")\n",
    "plt.title(\"Acurácia Relativa por Época\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        x = dataset[i].unsqueeze(0).to(device)\n",
    "        _, z = model(x)\n",
    "        embeddings.append(z.cpu().numpy().flatten())\n",
    "\n",
    "embeddings_df = pd.DataFrame(embeddings, columns=['z1', 'z2', 'z3'])\n",
    "result = pd.concat([df[['ID', 'Age', 'Sex', 'Class']], embeddings_df], axis=1)\n",
    "result.to_csv(r\"C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\joao\\data\\voice_embeddings.csv\", index=False)\n",
    "print(result.head())\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "classes = sorted(result['Class'].unique())\n",
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    subset = result[result['Class'] == cls]\n",
    "    ax.scatter(subset['z1'], subset['z2'], subset['z3'], \n",
    "    label=f\"Class {cls}\", alpha=0.7, color=colors[i % len(colors)])\n",
    "\n",
    "ax.set_xlabel(\"z1\")\n",
    "ax.set_ylabel(\"z2\")\n",
    "ax.set_zlabel(\"z3\")\n",
    "ax.set_title(\"Latent Space\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06d4d6",
   "metadata": {},
   "source": [
    "O objetivo principal de analisar o espaço latente é ver se o modelo aprendeu a representar classes diferentes em regiões distintas. \n",
    "\n",
    "<b>Separação entre classes</b>\n",
    "\n",
    "Como podem ver, as cores estão misturadas, o que indica que o espaço latente não está separando bem as classes. Isso sugere que o modelo ainda não capturou bem as características discriminativas entre as classes.\n",
    "\n",
    "<b>Distribuição e estrutura dos dados</b>\n",
    "\n",
    "A maior concentração dos pontos está perto da origem (em torno de (0,0) até (-2, 2)), o que é comum em autoencoders, pois o modelo tende a comprimir as representações para regiões centrais.\n",
    "Há alguns pontos fora desse núcleo, que provavelmente são os outliers ou amostras com características incomuns no conjunto. Esses pontos isolados às vezes revelam anomalias ou exemplos mal reconstruídos.\n",
    "\n",
    "<b>Densidade e sobreposição</b>\n",
    "\n",
    "A sobreposição de classes mostra que o modelo pode estar aprendendo características gerais, mas não específicas por classe ou que as classes não são facilmente separáveis nos dados originais.\n",
    "\n",
    "Temos duas hipóteses para isso:\n",
    "\n",
    "1. As classes realmente compartilham características semelhantes (pouca separabilidade nos dados brutos);\n",
    "\n",
    "2. O modelo tem capacidade insuficiente ou regularização excessiva, impedindo-o de criar fronteiras latentes mais distintas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
