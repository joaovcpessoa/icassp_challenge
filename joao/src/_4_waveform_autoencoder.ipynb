{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e742988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57f2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\joao\\data\\task1\\training\\phonationA'\n",
    "LABELS_PATH = r'C:\\Users\\joaov_zm1q2wh\\python\\icassp_challenge\\joao\\data\\task1\\labels.csv'\n",
    "SAMPLE_RATE = 8000\n",
    "WAVEFORM_INPUT_DIM = 1 \n",
    "LATENT_DIM = 20\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca66cd",
   "metadata": {},
   "source": [
    "### Sinal de áudio puro\n",
    "\n",
    "Quando você decide processar o sinal de áudio puro, o sinal é apenas uma sequência de amostras de amplitude no tempo.\n",
    "\n",
    "Em cada passo de tempo, o dado de entrada é um único valor numérico (a amplitude).\n",
    "\n",
    "$$\\text{Sinal de áudio: } [a_1, a_2, a_3, a_4, \\dots, a_N]$$\n",
    "\n",
    "Como o LSTM espera a dimensão: [Batch Size, Length, Feature Dimension]\n",
    "\n",
    "Precisamos moldar o sinal de áudio, que é 1D, para ter uma terceira dimensão (a dimensão da feature).\n",
    "\n",
    "O array de amostras $N$ é transformado em um array de vetores de dimensão 1, ou seja, $N \\times 1$.$$\\text{Novo Input: } [\\text{Batch Size}, \\text{N (amostras)}, \\mathbf{1}]$$Portanto, essa variável WAVEFORM_INPUT_DIM = 1 é como dizer para o modelo: \"Em cada passo de tempo da sequência, a entrada é um vetor de dimensão 1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7338f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioWaveformDataset(Dataset):\n",
    "    def __init__(self, audio_files, csv_path, sample_rate=8000, scaler=None):\n",
    "        self.audio_files = audio_files\n",
    "        self.sample_rate = sample_rate\n",
    "        self.scaler = scaler\n",
    "\n",
    "        self.df_labels = pd.read_csv(csv_path)\n",
    "        self.id_to_class = {row['ID']: int(row['Class']) for _, row in self.df_labels.iterrows()}\n",
    "\n",
    "        self.waveforms = []\n",
    "        self.classes = []\n",
    "\n",
    "        all_samples = []\n",
    "\n",
    "        print(\"Iniciando carregamento e processamento dos áudios...\")\n",
    "        for file in self.audio_files:\n",
    "            waveform = self.load_waveform(file)\n",
    "            if waveform is not None:\n",
    "                waveform = waveform.reshape(-1, WAVEFORM_INPUT_DIM) \n",
    "                \n",
    "                self.waveforms.append(waveform)\n",
    "                all_samples.append(waveform)\n",
    "\n",
    "                file_id = os.path.basename(file).split('_')[0]\n",
    "                if file_id not in self.id_to_class:\n",
    "                    raise ValueError(f\"ID {file_id} não encontrado no CSV\")\n",
    "                self.classes.append(self.id_to_class[file_id])\n",
    "\n",
    "        if self.scaler is None:\n",
    "            all_frames = np.concatenate(all_samples, axis=0) \n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(all_frames)\n",
    "            print(\"Scaler ajustado com sucesso no sinal puro (waveform).\")\n",
    "\n",
    "        self.normalized_waveforms = [\n",
    "            torch.tensor(self.scaler.transform(w), dtype=torch.float32)\n",
    "            for w in self.waveforms\n",
    "        ]\n",
    "\n",
    "        print(f\"Total de {len(self.normalized_waveforms)} áudios carregados e normalizados.\")\n",
    "\n",
    "    def load_waveform(self, file_path):\n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "            return audio\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.normalized_waveforms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform = self.normalized_waveforms[idx]\n",
    "        length = waveform.shape[0]\n",
    "        classe = self.classes[idx]\n",
    "        return waveform, length, classe\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, lengths, classes = zip(*batch)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    padded_batch = pad_sequence(sequences, batch_first=True)\n",
    "    \n",
    "    lengths, sorted_idx = lengths.sort(descending=True)\n",
    "    padded_batch = padded_batch[sorted_idx]\n",
    "    classes = torch.tensor([classes[i] for i in sorted_idx], dtype=torch.long)\n",
    "\n",
    "    return padded_batch, lengths, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d631651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder_lstm.flatten_parameters()\n",
    "        self.encoder_linear = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.decoder_linear = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder_lstm.flatten_parameters()\n",
    "\n",
    "        self.output_linear = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, max_len, _ = x.size()\n",
    "\n",
    "        packed_input = pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
    "        _, (h_n, c_n) = self.encoder_lstm(packed_input)\n",
    "        z = self.encoder_linear(h_n[-1])\n",
    "\n",
    "        h_0_decoder = self.decoder_linear(z).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c_0_decoder = torch.zeros_like(h_0_decoder)\n",
    "\n",
    "        packed_input_decoder = pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
    "        packed_output, _ = self.decoder_lstm(packed_input_decoder, (h_0_decoder, c_0_decoder))\n",
    "\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=max_len)\n",
    "        x_recon = self.output_linear(output)\n",
    "\n",
    "        return x_recon, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 272 arquivos de áudio.\n",
      "Iniciando carregamento e processamento dos áudios...\n",
      "Scaler ajustado com sucesso no sinal puro (waveform).\n",
      "Total de 272 áudios carregados e normalizados.\n",
      "Desabilitando cuDNN para evitar RuntimeError...\n",
      "\n",
      "Iniciando treinamento...\n",
      "For 1\n",
      "For 2\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 2\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 2\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 2\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 2\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 2\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 2\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n",
      "For 3\n"
     ]
    }
   ],
   "source": [
    "all_audio_files = glob.glob(os.path.join(AUDIO_DIR, \"*.wav\"))\n",
    "\n",
    "if not all_audio_files:\n",
    "    print(f\"Nenhum arquivo de áudio encontrado em {AUDIO_DIR}. Verifique o caminho e a extensão.\")\n",
    "else:\n",
    "    print(f\"Encontrados {len(all_audio_files)} arquivos de áudio.\")\n",
    "    \n",
    "    dataset = AudioWaveformDataset(all_audio_files, csv_path=LABELS_PATH, sample_rate=SAMPLE_RATE) \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = torch.device(\"cpu\")\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        print(\"Desabilitando cuDNN para evitar RuntimeError...\")\n",
    "        torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    model = LSTMAutoencoder(input_dim=WAVEFORM_INPUT_DIM, hidden_dim=64, latent_dim=LATENT_DIM).to(device)\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    loss_history = []\n",
    "    print(\"\\nIniciando treinamento...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        total_loss = 0\n",
    "        print('For 1')\n",
    "        for batch, lengths, classes in dataloader:\n",
    "            print('For 2')\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, z = model(batch, lengths)\n",
    "            \n",
    "            mask = torch.zeros_like(batch, dtype=torch.bool).to(device)\n",
    "            for i, l in enumerate(lengths):\n",
    "                print('For 3')\n",
    "                mask[i, :l, :] = True\n",
    "            \n",
    "            loss_all = criterion(recon, batch)\n",
    "            loss_masked = loss_all * mask.float() \n",
    "            loss = loss_masked.sum() / mask.sum() \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * lengths.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS} | Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "    print(\"Treinamento concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f466c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, N_EPOCHS+1), loss_history, marker=',')\n",
    "plt.title(\"Loss vs Épocas (Escala Logarítmica)\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals() and LATENT_DIM in [2, 3]:\n",
    "    print(\"\\nExtraindo vetores latentes...\")\n",
    "    \n",
    "    model.eval()\n",
    "    latent_vectors = []\n",
    "    all_classes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, lengths, classes in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            _, z = model(batch, lengths)\n",
    "            latent_vectors.append(z.cpu().numpy())\n",
    "            all_classes.append(classes.cpu().numpy())\n",
    "            \n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "    all_classes = np.concatenate(all_classes, axis=0)\n",
    "    \n",
    "    \n",
    "    unique_classes = np.unique(all_classes)\n",
    "    cmap = plt.cm.get_cmap('tab10', len(unique_classes))\n",
    "    class_to_idx = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "    colors = [cmap(class_to_idx[cls]) for cls in all_classes]\n",
    "        \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if LATENT_DIM == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        scatter = ax.scatter(latent_vectors[:, 0], latent_vectors[:, 1], latent_vectors[:, 2], c=colors, alpha=0.6)\n",
    "        \n",
    "        # Ajuste de limites para visualização\n",
    "        ax.set_xlim(latent_vectors[:, 0].min(), latent_vectors[:, 0].max())\n",
    "        ax.set_ylim(latent_vectors[:, 1].min(), latent_vectors[:, 1].max())\n",
    "        ax.set_zlim(latent_vectors[:, 2].min(), latent_vectors[:, 2].max())\n",
    "\n",
    "        ax.set_title('Espaço Latente 3D (Waveform)')\n",
    "        ax.set_xlabel('z1')\n",
    "        ax.set_ylabel('z2')\n",
    "        ax.set_zlabel('z3')\n",
    "    elif LATENT_DIM == 2:\n",
    "        ax = fig.add_subplot(111)\n",
    "        scatter = ax.scatter(latent_vectors[:, 0], latent_vectors[:, 1], c=colors, alpha=0.6)\n",
    "        ax.set_title('Espaço Latente 2D (Waveform)')\n",
    "        ax.set_xlabel('z1')\n",
    "        ax.set_ylabel('z2')\n",
    "    \n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(class_to_idx[cls]), markersize=10, label=f'Classe {cls}') for cls in unique_classes]\n",
    "    ax.legend(handles=handles, title=\"Classes\")\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "    print(f'Plotagem concluída. Total de {len(latent_vectors)} vetores latentes extraídos.')\n",
    "else:\n",
    "    print('Plotagem do espaço latente não realizada (LATENT_DIM deve ser 2 ou 3).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
